{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "5-f8-Il-O2o-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install necessary libraries"
      ],
      "metadata": {
        "id": "iM9-CcQqNJeN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install langchain_community tiktoken langchainhub langchain langchain-huggingface sentence_transformers chromadb  langchain-qdrant qdrant_client fastembed"
      ],
      "metadata": {
        "id": "9uKGCugsp_a4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Insert API key\n"
      ],
      "metadata": {
        "id": "1Y-7Ab8ANgQo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will need first to get an API key from [Google AI Studio](https://aistudio.google.com/app/apikey)."
      ],
      "metadata": {
        "id": "NDJTzUjpPbfI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "\n",
        "# GOOGLE_API_KEY = \"\"  # add your GOOGLE API key here\n",
        "# os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "\n",
        "#or run this\n",
        "from google.colab import userdata\n",
        "from google.colab import drive\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")"
      ],
      "metadata": {
        "id": "5OFp3k9gOny0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Indexing"
      ],
      "metadata": {
        "id": "xue1q_JwNTRG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 1. Load Data"
      ],
      "metadata": {
        "id": "CJXBK2WeNA6z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### From your files"
      ],
      "metadata": {
        "id": "91fbe_FeVkJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unstructured\n",
        "!pip install \"unstructured[pdf]\""
      ],
      "metadata": {
        "id": "T03YzOXPuLhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "# Create the folder\n",
        "folder_path = os.path.join(\"/content/\", \"uploaded_files\")\n",
        "os.makedirs(folder_path, exist_ok=True)  # Create if it doesn't exist\n",
        "\n",
        "# Upload files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Move uploaded files to the folder\n",
        "for filename, data in uploaded.items():\n",
        "  source_path = os.path.join(\"/content/\", filename)  # Path to uploaded file\n",
        "  destination_path = os.path.join(folder_path, filename)\n",
        "  shutil.move(source_path, destination_path)  # Move the file"
      ],
      "metadata": {
        "id": "e_tSe5C-YOX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader\n",
        "\n",
        "loader = DirectoryLoader(folder_path)\n",
        "docs = loader.load()\n",
        "len(docs)"
      ],
      "metadata": {
        "id": "LJF6Ekt6ybKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### From URLs"
      ],
      "metadata": {
        "id": "NRNXp47KVoB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4\n",
        "\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "# Load Documents\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        ")\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "3sB0fn_zXfoe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1f1c356-6c36-43fa-ad07-fe7e68a36812"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs)"
      ],
      "metadata": {
        "id": "K0JZNettypAc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcd29501-b538-4e64-9d37-4f1a887dfdf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Use a Text Splitter to Split Documents"
      ],
      "metadata": {
        "id": "jrOYMSUVbU3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=256, chunk_overlap=0)\n",
        "splits = text_splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "gOK-K7cNaoNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(splits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjpNB0r6u9St",
        "outputId": "bf736683-677e-4839-908b-d022a90b1a55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "241"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.Embed the documents and store them"
      ],
      "metadata": {
        "id": "0KRuEqoab-_z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load embedding model from Hugging Face"
      ],
      "metadata": {
        "id": "nSClfiwhYWVz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "#sentence-transformers/all-MiniLM-L6-v2\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    multi_process=True,\n",
        "    model_kwargs={\"device\": \"cpu\"},\n",
        "    encode_kwargs={\"normalize_embeddings\": True},  # Set `True` for cosine similarity\n",
        "\n",
        ")\n",
        "print(f\"Model's maximum sequence length: {SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2').max_seq_length}\")"
      ],
      "metadata": {
        "id": "fNeCHPd2biMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Connect to vector store locally , create collection and add the documents"
      ],
      "metadata": {
        "id": "YvCqqNKOYbBj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_qdrant import QdrantVectorStore\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http.models import Distance, VectorParams\n",
        "\n",
        "client = QdrantClient(\n",
        "    path=\"/content/vector_store_folder\"\n",
        "    #\":memory:\"\n",
        "    # you can use :memory: mode for fast and light-weight experiments,\n",
        ")\n",
        "client.create_collection(\n",
        "    collection_name=\"workshop_collection\",\n",
        "    vectors_config=VectorParams(size=384, distance=Distance.COSINE),\n",
        ")\n",
        "\n",
        "vector_store = QdrantVectorStore(\n",
        "    client=client,\n",
        "    collection_name=\"workshop_collection\",\n",
        "    embedding=embedding_model\n",
        ")\n",
        "vector_store.add_documents(documents=splits)\n"
      ],
      "metadata": {
        "id": "ZrXMzP0NjiP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Or connect to Qdrant Cloud collection with already indexed documents (hybrid search configuration)"
      ],
      "metadata": {
        "id": "0FRZW41Echku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_qdrant import FastEmbedSparse,QdrantVectorStore,RetrievalMode\n",
        "from qdrant_client import QdrantClient, models\n",
        "from qdrant_client.http.models import Distance, SparseVectorParams, VectorParams\n",
        "sparse_embeddings = FastEmbedSparse(model_name=\"Qdrant/bm25\")\n",
        "\n",
        "client = QdrantClient(\n",
        "    # set API KEY for Qdrant Cloud\n",
        "    url=\"https://e7739953-b688-421a-837b-6016c3420745.europe-west3-0.gcp.cloud.qdrant.io:6333\",\n",
        "    api_key=\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJyIiwiZXhwIjoxNzQ4MzA0OTE3fQ.DkuRnzWd704vhuq-fZJKZydO5genR1oHg2RBZuwD3Xo\",\n",
        ")\n",
        "vector_store = QdrantVectorStore(\n",
        "    client=client,\n",
        "    collection_name=\"workshop_collection\",\n",
        "    embedding=embedding_model,\n",
        "    sparse_embedding=sparse_embeddings,\n",
        "    retrieval_mode=RetrievalMode.HYBRID,\n",
        "    vector_name=\"dense\",\n",
        "    sparse_vector_name=\"sparse\",\n",
        ")"
      ],
      "metadata": {
        "id": "_9RsTgi2bRTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### In case qdrant doesnt work, use this:"
      ],
      "metadata": {
        "id": "v7Qj_xj-rv8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector_store = Chroma.from_documents(documents=splits,\n",
        "                                    embedding=embedding_model)"
      ],
      "metadata": {
        "id": "4Ejaa4S0gMLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval"
      ],
      "metadata": {
        "id": "XjiRMs8_sHy9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_query= \"What are the three core parts of an agent?\" #\"Which system became the first AI to earn an IMO medal?\""
      ],
      "metadata": {
        "id": "VLXUmPOfxy1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs = vector_store.similarity_search_with_score(query=user_query, k=5)"
      ],
      "metadata": {
        "id": "FyIyV99OhyQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs"
      ],
      "metadata": {
        "id": "xMTWq_V19nNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Content:\",retrieved_docs[0][0].page_content)\n",
        "print(\"Metadata\",retrieved_docs[0][0].metadata)\n",
        "print(\"Similarity score\",retrieved_docs[0][1])"
      ],
      "metadata": {
        "id": "PwuklVLs9lZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Retriever"
      ],
      "metadata": {
        "id": "152aw2qsd40c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vector_store.as_retriever(search_kwargs={\"k\": 15})\n",
        "docs = retriever.get_relevant_documents(user_query)\n",
        "print(docs[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myjSzHe0iKOB",
        "outputId": "118e1cae-bf6b-4c16-8002-06a9853ac4bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-c35d213439dc>:2: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  docs = retriever.get_relevant_documents(user_query)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Relationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\n",
            "Environment information is present in a tree structure.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generation"
      ],
      "metadata": {
        "id": "JAEDFpbfeAGk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define prompt"
      ],
      "metadata": {
        "id": "K6j14pgEeOoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "\n",
        "# Prompt\n",
        "\n",
        "system_prompt = \"\"\"You are an assistant for question-answering tasks.\n",
        "Given a user question and some retrieved article snippets, answer the user question.If the provided context doesn't contain the answer, answer from your knowledge but say that you do,else just say that you don't know, don't try to make up an answer.\n",
        "Here are the retrieved article snippets :\n",
        "{context}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_prompt),\n",
        "        (\"human\", \"{question}\"),\n",
        "    ]\n",
        ")\n"
      ],
      "metadata": {
        "id": "Zr_a-ocTymHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt.pretty_print()\n",
        "\n"
      ],
      "metadata": {
        "id": "N9KbK_0AlSzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configure LLM\n"
      ],
      "metadata": {
        "id": "5sEpfLmUeTKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install langchain-google-genai"
      ],
      "metadata": {
        "id": "L-1rGMxjpGi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-flash-preview-05-20\",#\"gemini-2.0-flash-lite\",#\"gemini-2.0-flash\",#\"gemini-2.5-flash-preview-04-17\",#\n",
        "    temperature=0,\n",
        "    # max_tokens=None,\n",
        "    # timeout=None,\n",
        "    # max_retries=2,\n",
        ")"
      ],
      "metadata": {
        "id": "EQx7GFjWOqso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create chain and invoke\n"
      ],
      "metadata": {
        "id": "2OOXZzMSeYc9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "\n",
        "# Post-processing\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "\n",
        "# Chain\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Question\n",
        "response=rag_chain.invoke(user_query)"
      ],
      "metadata": {
        "id": "RqhGAa6wqkv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "id": "lajBKQJBRx-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating the pipeline"
      ],
      "metadata": {
        "id": "A9ZswQfriusa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install ragas"
      ],
      "metadata": {
        "id": "9tYfuj5Yit8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ragas import EvaluationDataset\n",
        "\n",
        "# sample_queries = [\"What are the three core parts of an agent?\",\n",
        "#              \"Which hypothesis says a single reward can be enough for intelligence?\",\n",
        "#              \"Which system became the first AI to earn an IMO medal?\",\n",
        "#              \"What are the two basic multi-agent orchestration patterns?\",\n",
        "#              \"List the three steps in the guardrail-setup heuristic.\"\n",
        "#             ]\n",
        "\n",
        "# #If you are not interested in the context_recall metric, you don’t need to provide the ground_truths information.\n",
        "# expected_responses = [\"Model, Tools, Instructions\",\n",
        "#                 \"'Reward is Enough' hypothesis\",\n",
        "#                 \"AlphaProof\",\n",
        "#                 \"Manager pattern and Decentralised pattern\"],\n",
        "#                 \"1 Focus on privacy & safety 2 Add guardrails for real-world edge cases 3 Tune for both security and user experience\"\n",
        "#                  ]\n",
        "# #for sample website\n",
        "sample_queries = [\"What three components sit alongside the LLM “brain” in an autonomous agent system?\",\n",
        "             \"What are the two main kinds of memory an agent maintains?\",\n",
        "             \"Which prompting method tells the model to 'think step by step'?\",\n",
        "             \"Which framework fuses reasoning traces with discrete actions inside an agent?\",\n",
        "             \"Generative Agents score memories on three factors; name them.\"\n",
        "            ]\n",
        "\n",
        "#If you are not interested in the context_recall metric, you don’t need to provide the ground_truths information.\n",
        "expected_responses = [\"Planning, Memory, Tool use\",\n",
        "                \"Short-term memory and Long-term memory\",\n",
        "                \"Chain of Thought (CoT)\",\n",
        "                \"ReAct\",\n",
        "                \"Recency, Importance, Relevance\"\n",
        "                 ]\n",
        "\n",
        "dataset = []\n",
        "\n",
        "for query, reference in zip(sample_queries, expected_responses):\n",
        "    relevant_docs = retriever.invoke(query)\n",
        "    response = rag_chain.invoke(query)\n",
        "    dataset.append(\n",
        "        {\n",
        "            \"user_input\": query,\n",
        "            \"retrieved_contexts\": [rdoc.page_content for rdoc in relevant_docs],\n",
        "            \"response\": response,\n",
        "            \"reference\": reference,\n",
        "        }\n",
        "    )\n",
        "\n",
        "evaluation_dataset = EvaluationDataset.from_list(dataset)"
      ],
      "metadata": {
        "id": "dg_Nx4Rebt5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ragas import evaluate\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness\n",
        "\n",
        "evaluator_llm = LangchainLLMWrapper(llm)\n",
        "\n",
        "result = evaluate(\n",
        "    dataset=evaluation_dataset,\n",
        "    metrics=[LLMContextRecall(), Faithfulness(), FactualCorrectness()],\n",
        "    llm=evaluator_llm,\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "tltlkvNncyx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result)\n"
      ],
      "metadata": {
        "id": "1y0WAwE1c0IX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Citations"
      ],
      "metadata": {
        "id": "ZF_vXE7jmV-0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Change prompt"
      ],
      "metadata": {
        "id": "TSLCJTynn8S9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "\n",
        "# Prompt\n",
        "\n",
        "system_prompt = \"\"\"You are an assistant for question-answering tasks.\n",
        "Given a user question and some retrieved article snippets, answer the user question.If the provided context doesn't contain the answer, answer from your knowledge but say that you do,else just say that you don't know, don't try to make up an answer.\n",
        "Cite inline each snippet with\n",
        "When using information from the retrieved articles, cite your sources using the format [source id].For example, if you're using information from source id 1, cite it as [1].\n",
        "Only cite the most relevant sources that directly support your answer. The citations should be inline not in the end of your response.\n",
        "\n",
        "Here are the retrieved article snippets:\n",
        "{context}\n",
        "\"\"\"\n",
        "citations_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_prompt),\n",
        "        (\"human\", \"{question}\"),\n",
        "    ]\n",
        ")\n"
      ],
      "metadata": {
        "id": "WqaJcjV6n8S-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt.pretty_print()\n",
        "\n"
      ],
      "metadata": {
        "id": "lLMR4Sxsn8S_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Structured Output for the LLM"
      ],
      "metadata": {
        "id": "YQUmMPE6rDAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from typing_extensions import List, TypedDict\n",
        "\n",
        "\n",
        "class Citation(BaseModel):\n",
        "    source_id: int = Field(\n",
        "        ...,\n",
        "        description=\"The integer ID of a SPECIFIC source which justifies the answer.\",\n",
        "    )\n",
        "    source_name: str = Field(\n",
        "        ...,\n",
        "        description=\"The name of the source which justifies the answer.\",\n",
        "    )\n",
        "    quote: str = Field(\n",
        "        ...,\n",
        "        description=\"The VERBATIM quote from the specified source that justifies the answer.\",\n",
        "    )\n",
        "\n",
        "\n",
        "class QuotedAnswer(BaseModel):\n",
        "    \"\"\"Answer the user question based only on the given sources, and cite the sources used.\"\"\"\n",
        "\n",
        "    answer: str = Field(\n",
        "        ...,\n",
        "        description=\"The answer to the user question, which is based only on the given sources,which are also cited inline\",\n",
        "    )\n",
        "    citations: List[Citation] = Field(\n",
        "        ..., description=\"Citations from the given sources that justify the answer.\"\n",
        "    )"
      ],
      "metadata": {
        "id": "0z6TeDnTmaQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "structured_llm = llm.with_structured_output(QuotedAnswer)\n"
      ],
      "metadata": {
        "id": "K1ATQu02mjzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create chain and invoke\n"
      ],
      "metadata": {
        "id": "QkgY09y0oCZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "\n",
        "# Post-processing\n",
        "def format_docs(docs):\n",
        "    formatted_docs = []\n",
        "    for i, doc in enumerate(docs):  # Use enumerate to get the index\n",
        "        source = doc.metadata.get('source', 'Unknown Source')\n",
        "        # Include the index (source_id) before the source name\n",
        "        formatted_docs.append(f\"Source ID: {i+1}\\nSource: {source}\\nSnippet: {doc.page_content}\")\n",
        "    return \"\\n\\n\".join(formatted_docs)\n",
        "\n",
        "# Chain\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | citations_prompt\n",
        "    | structured_llm\n",
        ")\n",
        "\n",
        "# Question\n",
        "response=rag_chain.invoke(user_query)"
      ],
      "metadata": {
        "id": "XLeBhNBVoCZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.answer)\n",
        "for citation in response.citations:\n",
        "  cleaned_quote = citation.quote.replace('\\n', '')\n",
        "  print(f\"[{citation.source_id}] Quote: {cleaned_quote}. Source {citation.source_name}\")"
      ],
      "metadata": {
        "id": "EJDbjnix0hQn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}